<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Linear Discriminant Analysis</title>
    <link rel="stylesheet" href="/assets/css/styles.css">
  </head>
  <body>
    <nav>
    
    <a href=/ >
        Home
    </a>
    
    <a href=/about.html >
        About
    </a>
    
    <a href=/machinelearning.html >
        ML
    </a>
    
    <a href=/stats.html >
        Stats
    </a>
    
    <a href=/projects.html >
        Projects
    </a>
    
    <a href=/extra.html >
        Extra
    </a>
    
</nav>
    <script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
processEscapes: true},
jax: ["input/TeX","input/MathML","input/AsciiMath","output/CommonHTML"],
extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js","MathMenu.js","MathZoom.js","AssistiveMML.js", "[Contrib]/a11y/accessibility-menu.js"],
TeX: {
extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],
equationNumbers: {
autoNumber: "AMS"
}
}
});
</script>

<p><strong>Linear discriminant analysis</strong></p>

<p>We focus on the binary case, that is when $Y=+1$ or $Y=-1$, that is to
sets of variables.</p>

<p>These two conditional laws need to be gaussians with same covariance:</p>

<table>
  <tbody>
    <tr>
      <td>$X</td>
      <td>Y=+1 \sim \mathcal{N}(\mu_+,\Sigma)$ with density $f_+$</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>$X</td>
      <td>Y=-1 \sim \mathcal{N}(\mu_-,\Sigma)$ with density $f_-$</td>
    </tr>
  </tbody>
</table>

<p>Let $\pi_+$, $\pi_-$ be the simple probabilities $P(Y=+1)$, $P(Y=-1)$</p>

<table>
  <tbody>
    <tr>
      <td>$\mathbb{P}{Y=+1</td>
      <td>X=x} = \frac{\mathbb{P}{Y=+1, X=x}}{\mathbb{P}{X=x}}$</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>$\mathbb{P}{Y=+1</td>
      <td>X=x} = \frac{\mathbb{P}{X=x</td>
      <td>Y=+1} \mathbb{P}{Y=+1} }{\mathbb{P}{X=x} }$</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>$\mathbb{P}{Y=+1</td>
      <td>X=x} = \frac{f_+ \pi_+}{\mathbb{P}{X=x} }$</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>$\mathbb{P}{Y=+1</td>
      <td>X=x} = \frac{f_+ \pi_+}{(\mathbb{P}{X=x</td>
      <td>Y=+1}\mathbb{P}{Y= +1} + \mathbb{P}{X=x</td>
      <td>Y=-1}\mathbb{P}{Y= -1}) }$</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>$\mathbb{P}{Y=+1</td>
      <td>X=x} = \frac{f_+ \pi_+}{(f_+\pi_+ + f_-\pi_-)}$</td>
    </tr>
  </tbody>
</table>

<p>Similarly,</p>

<table>
  <tbody>
    <tr>
      <td>$\mathbb{P}{Y=-1</td>
      <td>X=x} = \frac{f_- (1-\pi_+)}{\mathbb{P}{X=x} }$</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>$\mathbb{P}{Y=-1</td>
      <td>X=x} = \frac{f_- (1-\pi_+)}{(f_+\pi_+ + f_-\pi_-)}$</td>
    </tr>
  </tbody>
</table>

<p>The result shows us that we can express the two conditionnal
probabilities in terms of conditionnal densities and "simple"
probabilities ($\pi_+$, $\pi_-$).</p>

<p>Recall that multivariable gaussian density is:
$f(x)=\frac{1}{\sqrt{2 \pi |\Sigma|}}e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)}$</p>

<p>In practice, $\mu_+$, $\mu_-$, $\pi_+$ and $\Sigma$ are unknown. Thus we
use empiric values:</p>

<p>$\widehat{\pi}_+ = m/n$</p>

<p>$\widehat{\mu}<em>+ = \frac{1}{m} \Sigma \mathbbm{1}</em>{{y_i=+1}}x_i$</p>

<p>$\widehat{\mu}<em>- = \frac{1}{n-m} \Sigma \mathbbm{1}</em>{{y_i=-1}}x_i$</p>

<p>$\widehat{\Sigma} = \frac{1}{n-2} ((m-1) \widehat{\Sigma}<em>+ + (n-m-1)\widehat{\Sigma}</em>-)$</p>

<p>$\widehat{\Sigma}<em>+ = \frac{1}{m-1} \Sigma \mathbbm{1}</em>{{y_i=+1}}(x_i-\widehat{\mu}<em>+)(x_i-\widehat{\mu}</em>+)^T$</p>

<p>$\widehat{\Sigma}<em>- = \frac{1}{n-m-1} \Sigma \mathbbm{1}</em>{{y_i=-1}}(x_i-\widehat{\mu}<em>-)(x_i-\widehat{\mu}</em>-)^T$</p>

<p>[Classification]{.underline}</p>

<table>
  <tbody>
    <tr>
      <td>We predict class = 1 when $\mathbb{P}(Y=+1</td>
      <td>X) &gt; \mathbb{P}(Y=-1</td>
      <td>X)$</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>=&gt; $\frac{\mathbb{P}(Y=+1</td>
      <td>X)}{\mathbb{P}(Y=-1</td>
      <td>X)} &gt; 1$</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>=&gt; $\log(\frac{\mathbb{P}(Y=+1</td>
      <td>X)}{\mathbb{P}(Y=-1</td>
      <td>X)}) &gt; 0$</td>
    </tr>
  </tbody>
</table>

<p>=&gt;</p>

<p>Using previous conditional probability expressions, we end up with the
following prediction rule:</p>

\[\begin{cases}
      1 &amp; \text{if}\ x^T\widehat{\Sigma}^{-1}(\widehat{\mu}_+ - \widehat{\mu}_-) &gt; \frac{1}{2}\widehat{\mu}_+^T\widehat{\Sigma}\widehat{\mu}_+ - \frac{1}{2}\widehat{\mu}_-^T\widehat{\Sigma}\widehat{\mu}_- + \log(1-m/n) - \log(m/n) \\
      -1 &amp; \text{otherwise}
    \end{cases}\]

<p>$\widehat{\mu}<em>+$, $\widehat{\mu}</em>-$, $\widehat{\pi}_+$ and
$\widehat{\Sigma}$ will be computed with <em>train data</em>.</p>

<p>$x$ is the <em>test data</em>.</p>

<p><em>Note</em>: $\widehat{\Sigma}^{-1}(\widehat{\mu}<em>+ - \widehat{\mu}</em>-)$ is
the <strong>Fisher function</strong> (Saporta).</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class LDAClassifier():
    
    def fit(self, X, y):       
        
        X_p = X[y == 1, :]
        X_m = X[y == -1, :]
        
        X_p_x1 = X_p[:,0]
        X_p_x2 = X_p[:,1]
        X_m_x1 = X_m[:,0]
        X_m_x2 = X_m[:,1]
        
        n = len(X)
        m = len(X_p)
        
        mean_p_x1 = np.mean(X_p_x1)
        mean_p_x2 = np.mean(X_p_x2)
        mean_p = np.array([mean_p_x1,mean_p_x2]) # mu_plus (estimated)
        cov_p = np.cov(np.transpose(X_p))
        

        mean_m_x1 = np.mean(X_m_x1)
        mean_m_x2 = np.mean(X_m_x2)
        mean_m = np.array([mean_m_x1,mean_m_x2]) # mu_minus (estimated)
        cov_m = np.cov(np.transpose(X_m))
        
        cov_est = (1/(n-2))*( (m-1)* cov_p + (n-m-1)* cov_m) # sigma (estimated)
        inv_cov_est = np.linalg.inv(cov_est)
        
        a1 = np.dot(np.transpose(mean_p),inv_cov_est)
        a2 = np.dot(np.transpose(mean_m),inv_cov_est)
        
        # 2nd term in inequality
        self.alpha = 0.5*(np.dot(a1,mean_p)  - 0.5*np.dot(a2,mean_m)) + np.log(1- m/n) - np.log(m/n)
        # 1st term in inequality
        self.beta =  np.dot(inv_cov_est,mean_p-mean_m)
        
        return self
    
    def predict(self, X):
        
        y_=[]
        
        for i in range(len(X)):
            X_pred = X[i]
            beta = np.dot(np.transpose(X_pred), self.beta)
            if (beta&gt;self.alpha):
                Y_pred = 1
            else:
                Y_pred = -1
            y_.append(Y_pred)
        return np.array(y_) 
</code></pre></div></div>

  </body>
</html>