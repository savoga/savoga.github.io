<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Em Gmm</title>
    <link rel="stylesheet" href="/assets/css/styles.css">
  </head>
  <body>
    <nav>
    
    <a href=/ >
        Home
    </a>
    
    <a href=/about.html >
        About
    </a>
    
    <a href=/machinelearning.html >
        ML
    </a>
    
    <a href=/stats.html >
        Stats
    </a>
    
    <a href=/projects.html >
        Projects
    </a>
    
    <a href=/extra.html >
        Extra
    </a>
    
</nav>
    <script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
processEscapes: true},
jax: ["input/TeX","input/MathML","input/AsciiMath","output/CommonHTML"],
extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js","MathMenu.js","MathZoom.js","AssistiveMML.js", "[Contrib]/a11y/accessibility-menu.js"],
TeX: {
extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],
equationNumbers: {
autoNumber: "AMS"
}
}
});
</script>

<p><strong>Expectation-Maximization (EM) in the case of GMM (Gaussian Mixture
Model)</strong></p>

<p>(for more details, see document <em>gmm.pdf</em> in Cloud folder)</p>

<p>GMM problem aims at estimating parameters of a sample distribution.
("Generative models" p. 295 [Understanding Machine
Learning]{.underline}).</p>

<p>A GMM sample is composed of $j$ Gaussian variables (<em>clusters</em>)
distributed with proportions $(\pi_1,…,\pi_k)$ ($\Sigma \pi_i =1$)</p>

<p>We can write:</p>

\[X \sim \mathcal{N}(\mu_{Z},\Sigma_{Z})~~~~~~with~~Z \sim \pi\]

<p>$\pi$ is not really a law but more the proportions of each Gaussian
categories.</p>

<p>Thus, $X$ has a density which is a weighted-average of all Gaussian
densities:</p>

\[p_\theta(x) = \Sigma_{j=1}^{k}\pi_j f_j(x)~~~~~~~(*)\]

<p>[Estimation]{.underline}</p>

<p>We want to estimate $\theta = (\pi, \mu, \Sigma)$ where:</p>

<p>$\pi=(\pi_1,…,\pi_k)$, $\mu=(\mu_1,…,\mu_k)$,
$\Sigma=(\Sigma_1,…,\Sigma_k)$</p>

<p>To do so, we use the maximum likelihood method (product of densities
across all samples):</p>

\[p_\theta(x)=\Pi_{i=1}^n p_\theta(x_i)\]

\[l(\theta)=log(\Pi_{i=1}^n p_\theta(x_i))=\Sigma_{i=1}^n log(p_\theta(x_i))\]

<p>We thus need to find $argmax(l(\theta))$</p>

<p>Problem: the likelihood function is not convex!</p>

<p>The expectation-maximization problem is used when we have <em>latent
variables</em> (= variables for which we don’t know their associated
distribution).</p>

<p>Let $z=(z_1,…,z_k)$ be the vector of latent variables. We can express
the density $(*)$ as a joint function with respect to $z$:</p>

\[p_\theta(x,z)=p_\theta(z)p_\theta(x|z)\]

\[l(\theta, z) = ... =\Sigma(log \pi_{z_i})+ \Sigma(logf_{z_i}(x_i))\]

<p>A classic optimization (in case of Gaussians) give us empirical values
as solutions e.g. $\hat{\pi_j}=\frac{n_j}{n}$</p>

<p>Problem: we don’t know $j$!</p>

<p>We will thus use the <em>expected</em> log-likelihood method.</p>

<p>Let us find another expression of the likelihood:</p>

\[p_\theta(x,z)=p_\theta(x)p_\theta(z|x)\]

<p>As seen previously: $p_\theta(x,z)=\Pi \pi_{z_i}f_{z_i}(x_i)$</p>

<table>
  <tbody>
    <tr>
      <td>$p_\theta(z</td>
      <td>x)=\Pi p_\theta(z_i</td>
      <td>x_i)=\frac{\Pi \pi_{z_i}f{z_i}(x_i)}{p_\theta(x_i)} \propto \Pi \pi_{z_i}f{z_i}(x_i)$</td>
    </tr>
  </tbody>
</table>

<p>Given an initial parameter $\theta_0$, the <em>expected</em> log-likelihood is
written as such:</p>

<p>\(\mathbb{E}_{\theta_0}[l(\theta;z)]=\Sigma p_{\theta_0}(z|x) l(\theta;z)\)
\(\mathbb{E}_{\theta_0}[l(\theta;z)]=\Sigma_{j} \Sigma_{i} p_{ij}(log\pi_j+logf_j(x_i))\)</p>

<p>We now have an expression that doesn’t depend on $z$ but only on
$p_{ij}$ and we know that $n_j=\Sigma_i p_{ij}$</p>

  </body>
</html>