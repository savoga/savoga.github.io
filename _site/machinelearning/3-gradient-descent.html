<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>3 Gradient Descent</title>
    <link rel="stylesheet" href="/assets/css/styles.css">
  </head>
  <body>
    <nav>
    
    <a href=/ >
        Home
    </a>
    
    <a href=/about.html >
        About
    </a>
    
    <a href=/machinelearning.html >
        ML
    </a>
    
    <a href=/stats.html >
        Stats
    </a>
    
    <a href=/projects.html >
        Projects
    </a>
    
    <a href=/extra.html >
        Extra
    </a>
    
</nav>
    <script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
processEscapes: true},
jax: ["input/TeX","input/MathML","input/AsciiMath","output/CommonHTML"],
extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js","MathMenu.js","MathZoom.js","AssistiveMML.js", "[Contrib]/a11y/accessibility-menu.js"],
TeX: {
extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],
equationNumbers: {
autoNumber: "AMS"
}
}
});
</script>

<p><strong>Gradient descent</strong></p>

<p>$\ell$ the loss function to minimize:</p>

<p>$\theta_{t+1}=\theta_t - \alpha \nabla \ell$</p>

<p>More details in Jupyter Notebook ML_GS.ipynb.</p>

<p>Proof of the formula:</p>

<p><img src="/assets/img/GradientDescent_proof.png" alt="image" height="20%" width="20%" /></p>

<p>(Idemia courses "DeepLearningTPT2018S1S2.pdf")</p>

<p>[Learning rate optimization]{.underline}</p>

<p>Note:</p>

<p>- one epoch = one forward pass and one backward pass of all the
training examples.</p>

<p>- batch size = the number of training examples in one forward/backward
pass. The higher the batch size, the more memory space needed.</p>

<p>- number of iterations = number of passes, each pass using [batch
size] number of examples. To be clear, one pass = one forward pass +
one backward pass.</p>

<p><em>Learning rate decay</em></p>

<p>Simple idea: reduce the learning rate progressively.</p>

<p>E.g. $1/t$ decay:</p>

\[\alpha_t=\frac{1}{(t+1)}\]

<p><em>Momentum</em></p>

<p>Momentum is a method that helps accelerate SGD in the relevant direction
and reduce oscillations.</p>

<p>General idea:</p>

<p>$0 \le \gamma \le 1$</p>

<p>$M_{t_0}=x_0$</p>

<p>$M_{t_1}=\gamma M_{t_0} + x_1$</p>

<p>$M_{t_2}=\gamma M_{t_1} + x_2$</p>

<p>Letâ€™s develop $M_{t_2}$ to have a better view of momentum effect:</p>

<p>$M_{t_2} = \gamma (\gamma M_{t_0} + x_1) + x_2 = \gamma^2 x_0 + \gamma x_1 + x_2$</p>

<p>We can see that more importance is given to the most recent value
($x_2$) and the least to the past values.</p>

<p>In practice, $\gamma = 0.9$ gives good results.</p>

<p><strong>Advantage</strong>: less dependent on noise</p>

<p><em>Adagrad - Adaptive Gradient Algorithm (2011)</em></p>

<p>Divide the learning rate by "average" gradient:</p>

<p>$\theta_t = \theta_{t-1} - \frac{\alpha}{\sqrt{\Sigma_{i=0}^t(\nabla f_i^2)}}\nabla f$</p>

<p><em>RMSProp - Root Mean Squared Propagation</em></p>

<p>Same as AdaGrad, but with an exponentially decaying average of past
squared gradients.</p>

<p>$\theta_t = \theta_{t-1} - \frac{\alpha}{\sigma_{t-1}}\nabla f$</p>

<p>Where:</p>

<p>$\sigma_{t} = \sqrt{\alpha (\sigma_{t-1})^2+(1-\alpha)(\nabla f_t)^2}$</p>

<p><em>Adam - Adaptive moment estimation</em></p>

<p>Adam = RMS + momentum =&gt; use of a exponential decaying average of past
[squared]{.underline} gradients and past gradients</p>

  </body>
</html>