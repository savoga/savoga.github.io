<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>1 Bayes Classifier</title>
    <link rel="stylesheet" href="/assets/css/styles.css">
  </head>
  <body>
    <nav>
    
    <a href=/ >
        Home
    </a>
    
    <a href=/about.html >
        About
    </a>
    
    <a href=/machinelearning.html >
        ML
    </a>
    
    <a href=/stats.html >
        Stats
    </a>
    
    <a href=/projects.html >
        Projects
    </a>
    
    <a href=/extra.html >
        Extra
    </a>
    
</nav>
    <script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
processEscapes: true},
jax: ["input/TeX","input/MathML","input/AsciiMath","output/CommonHTML"],
extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js","MathMenu.js","MathZoom.js","AssistiveMML.js", "[Contrib]/a11y/accessibility-menu.js"],
TeX: {
extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],
equationNumbers: {
autoNumber: "AMS"
}
}
});
</script>

<p>$\textbf{Bayes classifier}$</p>

<p><br /></p>

<p>$g$ is the <em>classifier</em>.</p>

<p>\(g: \mathcal{X} \to \mathcal{Y}\)
\(~~~~~~~~~~\mathbb{R}^d \to \{0,1\}\)</p>

<p>To model the learning problem, we use the pair $(X,Y)$ described by $(\mu, \eta)$ where $\mu$ is the probability measure:</p>

\[\mu(A) = \mathbb{P}(X \in A)\]

<p>And $\eta$ is the regression of $Y$ on $X$:</p>

\[\eta(X) = \mathbb{P}(Y=1 | X=x) = \mathbb{E}[Y | X=x]\]

<p>$\eta$ is also called the <em>a posteriori probability</em>.</p>

<p>The Bayes classifier is:</p>

\[\begin{cases}
      1 &amp; \text{if}\ \eta(x) &gt; 1/2 \\
      0 &amp; \text{otherwise}
    \end{cases}\]

<p>Or, if $\mathcal{Y}$ is ${-1,1}$, we write the classifier as such: \(g(x) = 2 \mathbb{1} \{ \eta(x)&gt;1/2 \}-1\).</p>

<p>$\underline{Theorem}$:</p>

<p>For any classifier g: \(\mathbb{R}^d \to \{0,1\}\),
\(\mathbb{P}(g^*(X) \neq Y) \le \mathbb{P}(g(X) \neq Y)\)</p>

<p>In other words, the Bayes classifier is theorically \textbf{the best classifier}.</p>

<p><em>Proof</em>: express $\mathbb{P}(g(X) \neq Y) - \mathbb{P}(g^*(X) \neq Y)$ in terms of dummies (use complementaries) and show that it is superior to 0.</p>

  </body>
</html>