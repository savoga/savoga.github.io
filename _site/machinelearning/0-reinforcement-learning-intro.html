<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>0 Reinforcement Learning Intro</title>
    <link rel="stylesheet" href="/assets/css/styles.css">
  </head>
  <body>
    <nav>
    
    <a href=/ >
        Home
    </a>
    
    <a href=/about.html >
        About
    </a>
    
    <a href=/machinelearning.html >
        ML
    </a>
    
    <a href=/stats.html >
        Stats
    </a>
    
    <a href=/projects.html >
        Projects
    </a>
    
    <a href=/extra.html >
        Extra
    </a>
    
</nav>
    <script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
processEscapes: true},
jax: ["input/TeX","input/MathML","input/AsciiMath","output/CommonHTML"],
extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js","MathMenu.js","MathZoom.js","AssistiveMML.js", "[Contrib]/a11y/accessibility-menu.js"],
TeX: {
extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],
equationNumbers: {
autoNumber: "AMS"
}
}
});
</script>

<p>Reinforcement learning is inspired on human logic: we learn which
strategy to take thanks to rewards we receive.</p>

<p>Reinforcement learning uses Markov Decision Processes (MDP).</p>

<p>A Markov Decision Process is defined by:</p>

<p>- an initial state $s_0$</p>

<table>
  <tbody>
    <tr>
      <td>- the reward distribution $r_t \sim p(r</td>
      <td>s_t, a_t)$ (stochastic)</td>
    </tr>
  </tbody>
</table>

<p>- the transition probabilities, $s_{t+1} \sim p(s | s_t, a_t)$
(stochastic)</p>

<p>=&gt; <strong>MDP: we know the state and the reward from the previous state
only</strong>.</p>

<p>A <em>policy</em> is an action for each state, for a given MDP.</p>

<p>=&gt; <strong>a policy can be seen as a strategy: we know where to go at each
state</strong>. In practice the policy is actually the <strong>transition probability
matrix</strong>.</p>

<p>The <em>value function</em> is the gain we earn at a state, for a specific
policy: $\forall s, V_\pi(s) = \mathbb{E}_{\pi}[G | s_0 = s]$</p>

<p>=&gt; <strong>The expected gain takes into account the transition probability.</strong></p>

<p>The gain is calculated as such:
$G = r_0 + \gamma r_1 + \gamma^2 r_2 +â€¦ = \Sigma_t \gamma^t r_t$ where
$\gamma$ is the <em>discount factor</em>.</p>

<p>$V$ can also be written
$V_\pi(s) = \mathbb{E}_\pi[r_0 + \gamma V(s_1) | s_0 = s]$. This
expression is called <strong>Bellman equation</strong>.</p>

<p>We can find the best policy:</p>

<table>
  <tbody>
    <tr>
      <td>$\forall s, \pi^<em>(s) = a^</em> \in argmax_a\mathbb{E}<em>\pi[r_0 + \gamma V</em>*(s_1)</td>
      <td>s_0 = s, a_0 = a]$</td>
    </tr>
  </tbody>
</table>

<p>Where $V_*$ is solution of the <strong>Bellman optimality equation</strong>:</p>

<table>
  <tbody>
    <tr>
      <td>$\forall s, V(s) = max_a\mathbb{E}[r_0 + \gamma V(s_1)</td>
      <td>s_0 = s]$</td>
    </tr>
  </tbody>
</table>

  </body>
</html>