I"ê<p><strong>Logistic regression</strong></p>

<p>Logistic regression is used for binary classification.</p>

<p>It is quite similar to a simple linear regression in the sense that the
objective is to find optimal weights $\omega$ to predict a variable.
However, in the logistic regression we use a sigmo√Ød function.<br />
Rem: "logistic" because the logistic law has a sigmo√Ød function as a
repartition function.<br />
[Rationale behind the use of the sigmo√Ød function]{.underline}:</p>

<p>We look for the <em>√† posteriori</em> probability
$\mathbb{P}(y=1 | x) = \pi (x) = \hat{y}$.</p>

<p>The predicted variable $\hat{y}$ is thus a probability.<br />
The sigmo√Ød function: $\sigma: z \to \frac{1}{1+e^{-z}}$ is well adapted
because of two reasons:</p>

<p>1) We want an output variable that is included in $[0,1]$<br />
2) $\frac{\pi(z)}{1-\pi(z)}$ represents the relationship between a
distribution and its complementary (good in binary case), and it is just
a transformation of $\sigma(z)=\frac{1}{1+e^{-z}}=\frac{e^z}{1-e^z}$<br />
Thus, we have:<br />
$\hat{y} = \mathbb{P}(y=1 | x) = \sigma(\omega ^Tx + b) = \frac{1}{1-e^{-(\omega ^Tx + b)}}$</p>

<p>[Estimation]{.underline}<br />
Estimation is done using maximum likelihood. Maximum likelihood is
finding the parameter that maximizes the probability to have a specific
event $(x_i, y_i)$ but in our case, it is a <em>conditional</em> maximum
likelihood since we want to maximize the <em>√† posteriori</em> probability that
depends on $x$.<br />
$L(\omega, b) = \prod_{i=1}^n \pi(x_i)^{y_i}(1-\pi(x_i))^{1-y_i}$<br />
This equation has no analytic solution. We use a numeric method to find
the optimal parameters (see optimizaton algorithms).</p>

<p>See <em>Neural Network</em> section for more details on optimization.</p>

<p>Note: logistic regression is really a linear model since the objective
is to find $\omega$ that is the slope of the line $\omega ^Tx + b$.</p>
:ET