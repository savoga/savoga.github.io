I"`<p>Reinforcement learning is inspired on human logic: we learn which
strategy to take thanks to rewards we receive.</p>

<p>Reinforcement learning uses Markov Decision Processes (MDP).</p>

<p>A Markov Decision Process is defined by:</p>

<p>- an initial state $s_0$</p>

<table>
  <tbody>
    <tr>
      <td>- the reward distribution $r_t \sim p(r</td>
      <td>s_t, a_t)$ (stochastic)</td>
    </tr>
  </tbody>
</table>

<p>- the transition probabilities, $s_{t+1} \sim p(s | s_t, a_t)$
(stochastic)</p>

<p>=&gt; <strong>MDP: we know the state and the reward from the previous state
only</strong>.</p>

<p>A <em>policy</em> is an action for each state, for a given MDP.</p>

<p>=&gt; <strong>a policy can be seen as a strategy: we know where to go at each
state</strong>. In practice the policy is actually the <strong>transition probability
matrix</strong>.</p>

<p>The <em>value function</em> is the gain we earn at a state, for a specific
policy: $\forall s, V_\pi(s) = \mathbb{E}_{\pi}[G | s_0 = s]$</p>

<p>=&gt; <strong>The expected gain takes into account the transition probability.</strong></p>

<p>The gain is calculated as such:
$G = r_0 + \gamma r_1 + \gamma^2 r_2 +â€¦ = \Sigma_t \gamma^t r_t$ where
$\gamma$ is the <em>discount factor</em>.</p>

<p>$V$ can also be written
$V_\pi(s) = \mathbb{E}_\pi[r_0 + \gamma V(s_1) | s_0 = s]$. This
expression is called <strong>Bellman equation</strong>.</p>

<p>We can find the best policy:</p>

<table>
  <tbody>
    <tr>
      <td>$\forall s, \pi^<em>(s) = a^</em> \in argmax_a\mathbb{E}<em>\pi[r_0 + \gamma V</em>*(s_1)</td>
      <td>s_0 = s, a_0 = a]$</td>
    </tr>
  </tbody>
</table>

<p>Where $V_*$ is solution of the <strong>Bellman optimality equation</strong>:</p>

<table>
  <tbody>
    <tr>
      <td>$\forall s, V(s) = max_a\mathbb{E}[r_0 + \gamma V(s_1)</td>
      <td>s_0 = s]$</td>
    </tr>
  </tbody>
</table>
:ET