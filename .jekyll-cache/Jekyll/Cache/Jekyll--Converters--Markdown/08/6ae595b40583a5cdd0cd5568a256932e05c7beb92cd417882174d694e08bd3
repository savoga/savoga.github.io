I"À
<p><strong>Expectation-Maximization (EM) in the case of GMM (Gaussian Mixture
Model)</strong></p>

<p>(for more details, see document <em>gmm.pdf</em> in Cloud folder)</p>

<p>GMM problem aims at estimating parameters of a sample distribution.
("Generative models" p. 295 [Understanding Machine
Learning]{.underline}).</p>

<p>A GMM sample is composed of $j$ Gaussian variables (<em>clusters</em>)
distributed with proportions $(\pi_1,â€¦,\pi_k)$ ($\Sigma \pi_i =1$)</p>

<p>We can write:</p>

\[X \sim \mathcal{N}(\mu_{Z},\Sigma_{Z})~~~~~~with~~Z \sim \pi\]

<p>$\pi$ is not really a law but more the proportions of each Gaussian
categories.</p>

<p>Thus, $X$ has a density which is a weighted-average of all Gaussian
densities:</p>

\[p_\theta(x) = \Sigma_{j=1}^{k}\pi_j f_j(x)~~~~~~~(*)\]

<p>[Estimation]{.underline}</p>

<p>We want to estimate $\theta = (\pi, \mu, \Sigma)$ where:</p>

<p>$\pi=(\pi_1,â€¦,\pi_k)$, $\mu=(\mu_1,â€¦,\mu_k)$,
$\Sigma=(\Sigma_1,â€¦,\Sigma_k)$</p>

<p>To do so, we use the maximum likelihood method (product of densities
across all samples):</p>

\[p_\theta(x)=\Pi_{i=1}^n p_\theta(x_i)\]

\[l(\theta)=log(\Pi_{i=1}^n p_\theta(x_i))=\Sigma_{i=1}^n log(p_\theta(x_i))\]

<p>We thus need to find $argmax(l(\theta))$</p>

<p>Problem: the likelihood function is not convex!</p>

<p>The expectation-maximization problem is used when we have <em>latent
variables</em> (= variables for which we donâ€™t know their associated
distribution).</p>

<p>Let $z=(z_1,â€¦,z_k)$ be the vector of latent variables. We can express
the density $(*)$ as a joint function with respect to $z$:</p>

\[p_\theta(x,z)=p_\theta(z)p_\theta(x|z)\]

\[l(\theta, z) = ... =\Sigma(log \pi_{z_i})+ \Sigma(logf_{z_i}(x_i))\]

<p>A classic optimization (in case of Gaussians) give us empirical values
as solutions e.g. $\hat{\pi_j}=\frac{n_j}{n}$</p>

<p>Problem: we donâ€™t know $j$!</p>

<p>We will thus use the <em>expected</em> log-likelihood method.</p>

<p>Let us find another expression of the likelihood:</p>

\[p_\theta(x,z)=p_\theta(x)p_\theta(z|x)\]

<p>As seen previously: $p_\theta(x,z)=\Pi \pi_{z_i}f_{z_i}(x_i)$</p>

<table>
  <tbody>
    <tr>
      <td>$p_\theta(z</td>
      <td>x)=\Pi p_\theta(z_i</td>
      <td>x_i)=\frac{\Pi \pi_{z_i}f{z_i}(x_i)}{p_\theta(x_i)} \propto \Pi \pi_{z_i}f{z_i}(x_i)$</td>
    </tr>
  </tbody>
</table>

<p>Given an initial parameter $\theta_0$, the <em>expected</em> log-likelihood is
written as such:</p>

<p>\(\mathbb{E}_{\theta_0}[l(\theta;z)]=\Sigma p_{\theta_0}(z|x) l(\theta;z)\)
\(\mathbb{E}_{\theta_0}[l(\theta;z)]=\Sigma_{j} \Sigma_{i} p_{ij}(log\pi_j+logf_j(x_i))\)</p>

<p>We now have an expression that doesnâ€™t depend on $z$ but only on
$p_{ij}$ and we know that $n_j=\Sigma_i p_{ij}$</p>
:ET