I"5	<p><strong>Gradient descent</strong></p>

<p>$\ell$ the loss function to minimize:</p>

<p>$\theta_{t+1}=\theta_t - \alpha \nabla \ell$</p>

<p>More details in Jupyter Notebook ML_GS.ipynb.</p>

<p>Proof of the formula:</p>

<p><img src="../images/GradientDescent_proof.png" alt="image" /></p>

<p>(Idemia courses "DeepLearningTPT2018S1S2.pdf")</p>

<p>[Learning rate optimization]{.underline}</p>

<p>Note:</p>

<p>- one epoch = one forward pass and one backward pass of all the
training examples.</p>

<p>- batch size = the number of training examples in one forward/backward
pass. The higher the batch size, the more memory space needed.</p>

<p>- number of iterations = number of passes, each pass using [batch
size] number of examples. To be clear, one pass = one forward pass +
one backward pass.</p>

<p><em>Learning rate decay</em></p>

<p>Simple idea: reduce the learning rate progressively.</p>

<p>E.g. $1/t$ decay:</p>

\[\alpha_t=\frac{1}{(t+1)}\]

<p><em>Momentum</em></p>

<p>Momentum is a method that helps accelerate SGD in the relevant direction
and reduce oscillations.</p>

<p>General idea:</p>

<p>$0 \le \gamma \le 1$</p>

<p>$M_{t_0}=x_0$</p>

<p>$M_{t_1}=\gamma M_{t_0} + x_1$</p>

<p>$M_{t_2}=\gamma M_{t_1} + x_2$</p>

<p>Letâ€™s develop $M_{t_2}$ to have a better view of momentum effect:</p>

<p>$M_{t_2} = \gamma (\gamma M_{t_0} + x_1) + x_2 = \gamma^2 x_0 + \gamma x_1 + x_2$</p>

<p>We can see that more importance is given to the most recent value
($x_2$) and the least to the past values.</p>

<p>In practice, $\gamma = 0.9$ gives good results.</p>

<p><strong>Advantage</strong>: less dependent on noise</p>

<p><em>Adagrad - Adaptive Gradient Algorithm (2011)</em></p>

<p>Divide the learning rate by "average" gradient:</p>

<p>$\theta_t = \theta_{t-1} - \frac{\alpha}{\sqrt{\Sigma_{i=0}^t(\nabla f_i^2)}}\nabla f$</p>

<p><em>RMSProp - Root Mean Squared Propagation</em></p>

<p>Same as AdaGrad, but with an exponentially decaying average of past
squared gradients.</p>

<p>$\theta_t = \theta_{t-1} - \frac{\alpha}{\sigma_{t-1}}\nabla f$</p>

<p>Where:</p>

<p>$\sigma_{t} = \sqrt{\alpha (\sigma_{t-1})^2+(1-\alpha)(\nabla f_t)^2}$</p>

<p><em>Adam - Adaptive moment estimation</em></p>

<p>Adam = RMS + momentum =&gt; use of a exponential decaying average of past
[squared]{.underline} gradients and past gradients</p>
:ET