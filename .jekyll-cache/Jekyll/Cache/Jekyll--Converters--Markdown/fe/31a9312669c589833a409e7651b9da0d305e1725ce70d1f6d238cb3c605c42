I"M<p><strong>AdaBoost</strong></p>

<p>Boosting is an algorithmic paradigm addressing two major issues in
machine learning:</p>

<p>- It optimizes the . The learning starts with a basic class (large
approximation error) and as it progresses the hypothesis class becomes
more complex.</p>

<p>- It allows to find predictors that are usually computationally
infeasible to find.</p>

<p>Main idea: weak learners are "boosted" to become stronger altogether.</p>

<p>![image]<a href="/assets/img/AdaBoost_Schema.png">scale=0.3</a></p>

<p>Weak learner (or $\gamma$-weak-learner): it’s an <strong>algorithm</strong> returning
a function $h$ such that $L_{\mathcal{D}}(h) \leq 1/2 - \gamma$. In
other words, it returns a simple binary predictor that does slightly
better than a random guess.</p>

<p>Input: training set $S=(x_1,y_1),…,(x_m,y_m)$, weak learner WL, number
of rounds $T$ Initialize $D^{(1)} = (1/m,…,1/m)$ // same weights for
all observations invoke weak learner $h_t = WL(D^{(t)},S)$ compute
$\epsilon_t = \Sigma_{i=1}^m D_i^{(t)} \mathbbm{1}<em>{[y_i \neq h_t(x_i)]}$
// misclassification error let
$\omega_t = \frac{1}{2} log(\frac{1}{\epsilon_t}-1)$ // $\epsilon_t &lt; 1$
update
$D_i^{(t+1)} = \frac{D_i^{(t)} exp(-\omega_t y_i h_t(x_i))}{\Sigma</em>{j=1}^m D_j^{(t)} exp(-\omega_t y_j h_t(x_j))}$
for all $i=1,…,m$ Output: the hypothesis
$h_s(x) = sign(\Sigma_{t=1}^T \omega_t h_t (x))$</p>

<p>We note:</p>

<p>- Final predictor = weighted sum of weak predictors</p>

<p>- More weights are given to observations that gave wrong prediction. In
doing so, the classifier of the next round will focus on these
observations. Warning: to see this, focus on the variation of
$D^{(t+1)}_i$ and not just $w_t$.</p>

<p>Theorem: the training error of the output hypothesis decreases
<strong>exponentially fast</strong> with the number of boosting rounds.</p>
:ET