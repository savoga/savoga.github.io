I"Õ<p><strong>Variational Auto-Encoder</strong></p>

<p>Variational autoencoders are a combination of three things:</p>

<p>1. Autoencoders</p>

<p>2. Variational Approximation &amp; Variational Lower Bound</p>

<p>3. "Reparameterization" Trick</p>

<p>1. Autoencoders</p>

<p>Autoencoders are used to extract features from unlabeled training data.
They are new methods for <strong>dimensionality reduction</strong> and part of neural
networks branch.</p>

<p><em>Note</em>: autoencoders can be used to replace older dimensionality
reduction methods such as PCA for several reasons:</p>

<p>- For very large data sets that can‚Äôt be stored in memory, PCA will not
be able to be performed. The autoencoder construction using keras can
easily be batched resolving memory limitations.</p>

<p>- PCA is restricted to linear separation while autoencoders are capable
of modelling complex non linear functions.</p>

<p><img src="Autoencoders.png" alt="image" /></p>

<p>Learning can be done using a loss function such as
$||x - \widehat{x}||^2$. In a similar way than neural network,
optimization is typically done with backpropagation.</p>

<p>2. Variational Approximation &amp; Variational Lower Bound</p>

<p>We assume $x$ is generated from unobserved (latent) $z$:</p>

<p><img src="decoder.png" alt="image" /></p>

<p><em>Practical example</em>: $x$ can be seen as images and $z$ as the main
attributes (orientation, colors, etc.)</p>

<p>$x \sim p_{\theta^<em>}(x | z)$ where $p_{\theta^</em>}(x | z)$ is called <em>true
conditional</em></p>

<p>$z \sim p_{\theta^<em>}(z)$ where $p_{\theta^</em>}(z)$ is called <em>true prior</em></p>

<p>Objective: estimating $p_{\theta}(x)$. We thus need to estimate
$\theta^*$.</p>

<p>We can do it through maximum likelihood. The marginal density is
$p_{\theta}(x) = \int p_{\theta}(x|z) p_{\theta}(z) dz$</p>

<p><em>Note</em>: a marginal likelihood function is a likelihood function in which
some parameter variables have been marginalized. Marginalization
consists in summing over the possible values of one variable in order to
determine the contribution of another. E.g.,
$\mathbb{P}(X)=\Sigma_y \mathbb{P}(X, Y=y)$ or in continuous
probabilities $p(x)=\int p(x, y) dy$. Also, if we don‚Äôt know the joint
probability, we can express this using conditional probabilities:
$p(x)=\int p(x | y) p(y) dy$</p>

<p>Problem: impossible to compute $p(x|z)$ for every $z$ ([computationally
too expensive]{style=‚Äùcolor: orange‚Äù}) =&gt; problem is said
<strong>intractable</strong></p>

<table>
  <tbody>
    <tr>
      <td>Solution: use another encoder learning $q_\phi (z</td>
      <td>x)$ that approximates</td>
    </tr>
    <tr>
      <td>$p_\theta(z</td>
      <td>x)$</td>
    </tr>
  </tbody>
</table>

<p><img src="log-likelihood-VAE.png" alt="image" /></p>

<p>$\mathbb{E}<em>z[\log p</em>\theta (x^{(i)} | z]$: we can estimate this term
through sampling</p>

<table>
  <tbody>
    <tr>
      <td>$D_{KL}(q_\phi(z</td>
      <td>x^{(i)})</td>
      <td>¬†</td>
      <td>p_\theta(z))$: differentiable term</td>
    </tr>
  </tbody>
</table>

<p>$D_{KL}(q_\phi(z | x^{(i)}) || p_\theta(z | x^{(i)}))$: $p(z|x)$
intractable but we know that $D_{KL} \geq 0$</p>

<p>Let
$\mathcal{L}(x^{(i)}, \theta, \phi) = \mathbb{E}<em>z[\log p</em>\theta (x^{(i)} | z] - D_{KL}(q_\phi(z | x^{(i)}) || p_\theta(z))$
= <strong>tractable lower bound</strong> that we can optimize</p>

<p>We know that
$p_\theta (x^{(i)}) \geq \mathcal{L}(x^{(i)}, \theta, \phi)$ since
$D_{KL}(q_\phi(z | x^{(i)}) || p_\theta(z | x^{(i)})) \geq 0$</p>

<p>Thus the maximum likelihood problem becomes:
$\theta^<em>, \phi^</em> = argmax_{\theta, \phi} \Sigma_{i=1}^N \mathcal{L}(x^{(i)}, \theta, \phi)$</p>

<p>We can minimize $D_{KL}(q_\phi(z | x^{(i)}) || p_\theta(z))$ making
posterior distribution close to prior. To do so, we make encoder network
predicting $\mu_{z | x}$ and $\Sigma_{z | x}$ and then we sample
$z | x \sim \mathcal{N}(\mu_{z | x}, \Sigma_{z | x})$</p>

<p><img src="VAE_final_schema.png" alt="image" /></p>

<p>Problem: sampling $z | x \sim \mathcal{N}(\mu_{z | x}, \Sigma_{z | x})$
and $x | z \sim \mathcal{N}(\mu_{x | z}, \Sigma_{x | z})$ is not
differentiable ([why?]{style=‚Äùcolor: orange‚Äù}).</p>

<p>=&gt; we use <strong>reparametrization trick</strong>: we sample
$z_0 \sim \mathcal{N}(0,1)$ to have
$z = \mu_{x | z} + z_0 \Sigma_{x | z} \sim \mathcal{N}(\mu_{x | z}, \Sigma_{x | z})$</p>

<p><img src="VAE_reparametrization_trick.png" alt="image" /></p>

<p>Optimization through forward and backward propagation!</p>
:ET