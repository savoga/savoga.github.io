I"<p><strong>Q-learning</strong></p>

<p>This algorithm is also based on $\epsilon$-greedy algorithm.</p>

\[\pi(s) \leftarrow
    \begin{cases}
     a^* \in argmax_a Q(s,a) $ with probability $ 1-\epsilon \\
    random $ with probability $ \epsilon
    \end{cases}\]

<p>\(\underline{Estimation}\): unlike SARSA, Q-learning aims at updating the estimator
using the best action at each iteration:</p>

<p>$\forall t, Q(s_t, a_t) \xleftarrow{\alpha} r_t + \gamma max_{a} Q(s_{t+1}, a)$</p>

<p>The only modification from SARSA is the following line:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Q[state_prev, action_prev] = (1 - alpha) *  Q[state_prev, action_prev] + alpha * (rewards[action_prev] + gamma * np.max(Q[state].data))
</code></pre></div></div>
:ET